# FastLLM: A Custom Implementation of a Large Language Model with CUDA
## Introduction
Welcome to FastLLM, a project dedicated to building a Large Language Model (LLM) from scratch using C++ and CUDA. FastLLM aims to provide an educational platform for beginners and enthusiasts to learn about the inner workings of LLMs and the optimization techniques used in high-performance computing with CUDA. FastLLM is inspired by vLLM but is designed to offer a hands-on approach to understanding LLMs by implementing each component manually.

## Project Objectives
- Understand LLMs: 
Gain a deep understanding of the architecture and components of Large Language Models.
- Learn CUDA: 
Explore CUDA programming for accelerating matrix operations and other computational tasks.
- Build from Scratch: 
Implement the core components of an LLM, including tokenization, attention mechanisms, and model inference.
- Optimize Performance: 
Use CUDA to optimize the performance of the model, focusing on parallelization and efficient memory usage.

## Roadmap
- implement the llm operators
- implement the kv cache
- add flash attention
- add inflight batch

# Contact
For questions, feel free to reach out via email at johny@vip.qq.com